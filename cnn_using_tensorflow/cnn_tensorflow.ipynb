{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib.request as url\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# download cifar\n",
    "check if CIFAR in directory, if not, download "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data():\n",
    "    if not os.path.exists('.././data'):\n",
    "        os.mkdir('.././data')\n",
    "    if not os.path.exists('.././data/cifar-10-python.tar.gz'):\n",
    "        print(\"CIFAR-10 not found, downloading...\")\n",
    "        url.urlretrieve(\"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\",\n",
    "                        \".././data/cifar-10-python.tar.gz\")\n",
    "        print(\"done\")\n",
    "        print(\"Download completed!\")\n",
    "    else:\n",
    "        print('CIFAR-10 already exists.')\n",
    "        \n",
    "# download_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data_batch_1', 'data_batch_4', 'data_batch_3', 'data_batch_2', 'data_batch_5']\n",
      "(49000, 3072) (49000,)\n",
      "(1000, 3072) (1000,)\n",
      "(10000, 3072) (10000,)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import _pickle as pickle\n",
    "def load_data():\n",
    "    if not os.path.exists('.././data/cifar-10-python.tar.gz'):\n",
    "        download_data()\n",
    "    if not os.path.exists('.././data/cifar-10-batches-py/'):\n",
    "        print(\"extracting...\")\n",
    "        package = tarfile.open('.././data/cifar-10-python.tar.gz')\n",
    "        package.extractall('.././data')\n",
    "        package.close()\n",
    "        \n",
    "    root_dir = os.getcwd()\n",
    "    os.chdir('.././data/cifar-10-batches-py')\n",
    "    train_data = []\n",
    "    train_label = []\n",
    "    test_data = []\n",
    "    test_label = []\n",
    "    data_train = glob.glob('data_batch*')\n",
    "    print(data_train)\n",
    "    #try:\n",
    "    for name in data_train:\n",
    "        handle = open(name, 'rb')\n",
    "        cmap = pickle.load(handle, encoding='bytes')\n",
    "        train_data.append(cmap[b'data'])\n",
    "        train_label.append(cmap[b'labels'])\n",
    "        handle.close()\n",
    "    # Turn the dataset into numpy compatible arrays.\n",
    "    train_data = np.concatenate(train_data, axis=0)\n",
    "    train_label = np.concatenate(train_label)\n",
    "    handle = open('test_batch', 'rb')\n",
    "    cmap = pickle.load(handle, encoding='bytes')\n",
    "    test_data.append(cmap[b'data'])\n",
    "    test_label.append(cmap[b'labels'])\n",
    "    test_data = np.array(test_data[0])\n",
    "    test_label = np.array(test_label[0])\n",
    "    #except BaseException:\n",
    "#         os.chdir(root_dir)\n",
    "#         print('Something went wrong...')\n",
    "#         return None\n",
    "    os.chdir(root_dir)\n",
    "    return train_data,train_label,test_data,test_label\n",
    "\n",
    "train_data,train_label,test_data,test_label = load_data()\n",
    "val_size = 1000\n",
    "val_data = train_data[-1000:]\n",
    "val_label = train_label[-1000:]\n",
    "train_data = train_data[:-1000]\n",
    "train_label = train_label [:-1000]\n",
    "print(train_data.shape, train_label.shape)\n",
    "print(val_data.shape, val_label.shape)\n",
    "print(test_data.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build neural network using tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EstimatorSpec(predictions={}, loss=<tf.Tensor 'sparse_softmax_cross_entropy_loss/value:0' shape=() dtype=float32>, train_op=<tf.Operation 'GradientDescent' type=NoOp>, eval_metric_ops={}, export_outputs=None, training_chief_hooks=(), training_hooks=(), scaffold=<tensorflow.python.training.monitored_session.Scaffold object at 0x7f0c21e972b0>)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "def cnn_4_cifar(features, labels, mode = tf.estimator.ModeKeys.TRAIN):\n",
    "    # features aka. x, labels aka y, i don't know mode right now\n",
    "    train_x = features\n",
    "    train_y = labels\n",
    "    input_data = tf.placeholder(\n",
    "        dtype = tf.float32,\n",
    "        shape = [None, 3072],\n",
    "        name = \"input\")\n",
    "    \n",
    "    labels = tf.placeholder(\n",
    "        dtype = tf.int32,\n",
    "        shape = [None, ],\n",
    "        name = \"labels\")\n",
    "    \n",
    "    input_labels = tf.one_hot(indices=labels,\n",
    "                        depth = 10,\n",
    "                        name='one_hot_label',\n",
    "                        axis=1,\n",
    "                       dtype = tf.int32)\n",
    "    \n",
    "    ###### input layer ######\n",
    "    \n",
    "    input_layer = tf.reshape(input_data, [-1, 32, 32, 3])\n",
    "    input_layer = tf.cast(input_layer, dtype = tf.float32)\n",
    "    \n",
    "    ###### normalize layer ######\n",
    "    \n",
    "    normal_layer = tf.layers.batch_normalization(\n",
    "        inputs = input_layer,\n",
    "        training= mode == tf.estimator.ModeKeys.TRAIN,\n",
    "        reuse=None,\n",
    "        name = 'batch_normal'\n",
    "    )\n",
    "    #normalize_layer = tf.nn.l2_normalize()\n",
    "    \n",
    "    ###### convolutional layer #1 ######\n",
    "    \n",
    "    conv1 = tf.layers.conv2d(\n",
    "        inputs = normal_layer,\n",
    "        filters = 18,\n",
    "        kernel_size = [3,3],\n",
    "        padding = \"same\",\n",
    "        activation = tf.nn.relu\n",
    "    )\n",
    "    \n",
    "    ###### pooling layer #1 ######\n",
    "    \n",
    "    pool1 = tf.layers.max_pooling2d(\n",
    "        inputs=conv1, pool_size = [2,2], strides = 2)\n",
    "    \n",
    "    \n",
    "    ###### convolutional layer #2 ######\n",
    "        \n",
    "    conv2 = tf.layers.conv2d(\n",
    "        inputs = pool1,\n",
    "        filters = 64,\n",
    "        kernel_size = [3,3],\n",
    "        padding = \"same\",\n",
    "        activation = tf.nn.relu\n",
    "    )\n",
    "    \n",
    "    ###### pooling layer #1 ######\n",
    "    \n",
    "    pool2 = tf.layers.max_pooling2d(\n",
    "        inputs = conv2,\n",
    "        pool_size = [2,2],\n",
    "        strides = 2)\n",
    "    \n",
    "    ###### flat layer ######\n",
    "    \n",
    "    pool_flat = tf.reshape(pool2, [-1, 8 * 8 * 64])\n",
    "    \n",
    "    ###### dense layer ######\n",
    "    \n",
    "    dense1 = tf.layers.dense(\n",
    "        inputs = pool_flat,                     \n",
    "        units = 1024,                     \n",
    "        activation = tf.nn.relu\n",
    "    )\n",
    "    \n",
    "    dense2 = tf.layers.dense(\n",
    "        inputs = dense1,                     \n",
    "        units = 200,                     \n",
    "        activation = tf.nn.relu\n",
    "    )\n",
    "    \n",
    "    ###### dropout layer ######\n",
    "    \n",
    "    dropout = tf.layers.dropout(\n",
    "        inputs = dense2,\n",
    "        rate = 0.2,\n",
    "        training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    )\n",
    "    ###### logits layer ######\n",
    "    \n",
    "    logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "    \n",
    "    ###### prediction ######\n",
    "    \n",
    "    predictions = {\n",
    "        \"class\" : tf.argmax(input = logits, axis = 1),\n",
    "        \"prob\" : tf.nn.softmax(\n",
    "            logits, \n",
    "            name = \"softmax_tensor\")\n",
    "    }\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode, \n",
    "            predictions = predictions)\n",
    "    \n",
    "    ###### loss ######\n",
    "    \n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(\n",
    "        labels = labels,\n",
    "        logits = logits\n",
    "    )\n",
    "    \n",
    "    ###### train op ######\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(\n",
    "            learning_rate = 0.001\n",
    "        )\n",
    "        train_op = optimizer.minimize(\n",
    "            loss = loss,\n",
    "            global_step = tf.train.get_global_step()\n",
    "        )\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode = mode,\n",
    "            loss = loss,\n",
    "            train_op = train_op\n",
    "            )\n",
    "    \n",
    "    ##### eval op ######\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        eval_metric_ops = {\n",
    "            \"accuracy\": tf.metrics.accuracy(\n",
    "                labels=labels,\n",
    "                predictions=predictions[\"classes\"])}\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        loss=loss,\n",
    "        eval_metric_ops=eval_metric_ops)\n",
    "    \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth=True\n",
    "    sess = tf.Session(config=config)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    return sess.run(logits, feed_dict =\n",
    "                    {input_data: train_x,\n",
    "                    labels: train_y})\n",
    "\n",
    "\n",
    "\n",
    "print(cnn_4_cifar(train_data[0:2], train_label[0:2]))\n",
    "\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.allow_growth=True\n",
    "# sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/mnist_convnet_model', '_tf_random_seed': 1, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100}\n",
      "Tensor(\"Shape_4:0\", shape=(2,), dtype=int32)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'Dimension'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-259f09a9d304>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     hooks=[logging_hook])\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ht-dl-gpu/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps)\u001b[0m\n\u001b[1;32m    239\u001b[0m       \u001b[0mhooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStopAtStepHook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ht-dl-gpu/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks)\u001b[0m\n\u001b[1;32m    626\u001b[0m       \u001b[0mglobal_step_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_and_assert_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m       features, labels = self._get_features_and_labels_from_input_fn(\n\u001b[0;32m--> 628\u001b[0;31m           input_fn, model_fn_lib.ModeKeys.TRAIN)\n\u001b[0m\u001b[1;32m    629\u001b[0m       estimator_spec = self._call_model_fn(features, labels,\n\u001b[1;32m    630\u001b[0m                                            model_fn_lib.ModeKeys.TRAIN)\n",
      "\u001b[0;32m~/anaconda3/envs/ht-dl-gpu/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_get_features_and_labels_from_input_fn\u001b[0;34m(self, input_fn, mode)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_get_features_and_labels_from_input_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_input_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ht-dl-gpu/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_input_fn\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    583\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/cpu:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ht-dl-gpu/lib/python3.6/site-packages/tensorflow/python/estimator/inputs/numpy_io.py\u001b[0m in \u001b[0;36minput_fn\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m       \u001b[0mordered_dict_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munique_target_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mordered_dict_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m       shape_dict_of_x = {k: ordered_dict_x[k].shape\n\u001b[1;32m    111\u001b[0m                          for k in ordered_dict_x.keys()}\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'Dimension'"
     ]
    }
   ],
   "source": [
    "###### estimator ######\n",
    "\n",
    "cifar_classifier = tf.estimator.Estimator(model_fn = cnn_4_cifar, model_dir=\"/tmp/mnist_convnet_model\")\n",
    "\n",
    "###### log ######\n",
    "\n",
    "tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "logging_hook = tf.train.LoggingTensorHook(\n",
    "    tensors=tensors_to_log, every_n_iter=50)\n",
    "\n",
    "###### training ######\n",
    "tf.reshape(train_data, [-1, 3072])\n",
    "\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"input\": train_data},\n",
    "    y=train_label,\n",
    "    batch_size=100,\n",
    "    num_epochs=None,\n",
    "    shuffle=True)\n",
    "cifar_classifier.train(\n",
    "    input_fn=train_input_fn,\n",
    "    steps=20000,\n",
    "    hooks=[logging_hook])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
